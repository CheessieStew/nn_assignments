{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent neural networks\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Up to now we used neural networks to learn functions that predicted a desired output $y$ based on an input vector $x$. We have assumed that $x$ has a constant size, such as an image of a given resolution.\n",
    "\n",
    "In contrast, recurrent networks can be applied to sequences of different length.\n",
    "\n",
    "Consider the problem of computing the parity checksum of a sequence of bits. Traditional neural networks that we know can not be applied to bit sequences of arbitrary length, yet a program which reads in the sequence bit-by-bit is very easy to write. \n",
    "\n",
    "Another motivating example is sequence generation, such as generating text or music. Again, normal neural networks that take a fixed input size are not directly applicable to whole sequences.\n",
    "\n",
    "## Autoregressive models\n",
    "\n",
    "An autoregressive model assumes, that the $i$-th element of a sequence depends only on a few preceding sequence elements. Lets first write the exact probability of observing a sequence $\\mathbf{Y} = [Y_1, Y_2, \\ldots, Y_m]$:\n",
    "\n",
    "$$\n",
    "    P(\\mathbf{Y}) = P(Y_1)P(Y_2|Y_1)P(Y_3|Y_1, Y_2)\\ldots P(Y_m|Y1,Y_2,\\ldots,Y_{n-1})=\\prod_i P(Y_i|Y_1,\\ldots, Y_{i-1})\n",
    "$$\n",
    "\n",
    "The autoregressive model simply assumes that we can use only a finite history, that is:\n",
    "$$\n",
    "    P(Y_i|Y_1,\\ldots, Y_{i-1}) \\approx P(Y_i|Y_{i-n},\\ldots, Y_{i-1})\n",
    "$$\n",
    "\n",
    "Autoregressive models are quite frequently used, because they are easy to implement - it is sufficient extract $n$ element long subsequences and learn a model that predicts the last element given the $n-1$ preceding ones. Thus autoregressive models reduce the sequence learning problem to the typicall supervised learning setup\n",
    "\n",
    "### Examples\n",
    "\n",
    "Autoregressive models are very popular, for instance:\n",
    "\n",
    "1. Language models (that is models which tell how probable is a given utterance) are often expressed as $n$-gram models in which $P(Y_i|Y_{i-n},\\ldots, Y_{i-1})$ is simply established by counting occurrences in a corpus of text.\n",
    "2. The ARMA model used in timeseries prediction is expressed as:\n",
    "    $$\n",
    "        Y_t = \\sum_{i=1}^{n}\\alpha_i Y_{t-i} + \\sum_{i=1}^k \\Theta_i \\epsilon_{t-i},\n",
    "    $$\n",
    "    where $\\epsilon_i$ are assumed to be normally distributed noise variables.\n",
    "    \n",
    "## Models with a hidden state\n",
    "\n",
    "Autoregressive models have a very short memory which limits their applicability. We can build a more powerfull model by introducing a sequence of hidden states $\\mathbf{H} = [H_0, H_1, \\ldots]$. We will assume that all history of a sequence can be captured by the state:\n",
    "\n",
    "$$\n",
    "    P(Y_i|Y_1,\\ldots, Y_{i-1}) \\approx P(Y_i|Y_{i-n}, H_{i-1})\n",
    "$$\n",
    "\n",
    "\n",
    "### Recurrent neural networks\n",
    "\n",
    "Recurrent neural networks are a generalization of the model with the hidden state. We will assume that there is an input sequence $\\mathbb{X}$. The network will process the elements of $\\mathbb{X}$  one at a time producing a sequence of hidden states and a sequence of outputs. We will train the model by specifying the desired outputs. We will be able to supervise the model at each step (which is common e.g. in sequence generators which are taught to predict the next sequence element) or only at the end (which can be used in the parity computing network).\n",
    "\n",
    "Define a recurrent computation:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    H_t &= f(X_t, H_{t-1}) \\\\\n",
    "    Y_t &= g(X_t, H_t)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In a recurrent neural network the function $f$ and $g$ are implemented as multilayer neural networks.\n",
    "\n",
    "### Backpropagation through time\n",
    "\n",
    "The last question is how to train such a recurrent network? The typicall approach is to *unroll the network in time*, then compute the loss and backpropagate it over the time steps. Gradient backpropagation algorithm works, thus in principle training doesn't require new skill. Hovewer, the unrolled network is very deep (its depth equals to the number of time steps!) and quite pathological, because the same weight matrices are reused at all times. For this reasons recurrent networks suffer from two problems:\n",
    "\n",
    "1. Gradient vanishing, when the gradient drops to zero and no training is possible.\n",
    "2. Gradient explosion, when the gradient grows really quickly and a single step can destroy the network.\n",
    "\n",
    "The two problems are related. Consider the following recurrency:\n",
    "\n",
    "$$\n",
    "    H_t = W_{hh}H_{t-1} = (W_{hh})^t H_0\n",
    "$$\n",
    "\n",
    "Suppose $H_0$ is an eigenvector - then depending on the magnitude of the associated eigenvalue the hidden state will either exponentially grow or decay! Of course, the network will typically have sone nonlinearities that will prevent the explosion of the hiden state. However, the backpropagation computation is linear (because all nonlinearities are linerized at the operating point). Thus the gradient is very prone to explode or implode!\n",
    "\n",
    "### Solutions to gradient pathologies\n",
    "\n",
    "#### Echo-state networks\n",
    "The first solution to training recurrent networks is... not to train the recurrent connections! This approach is taken in the *echo state networks* which perform the following computation:\n",
    "\n",
    "$$\n",
    "    \\begin{align}\n",
    "        H_t &= \\tanh(W_{xh}X_t + W_{hh} H_{t-1} + b_h) \\\\\n",
    "        O_t &= W_{xo}X_t + W_{ho}H_T + b_o\n",
    "    \\end{align}\n",
    "$$\n",
    "\n",
    "The training procedure is as follows:\n",
    "\n",
    "1. Randomly sample $W_{hh}$ and $W_{xh}$.\n",
    "2. Rescale $W_{hh}$ to have the largest eigenvalue close to 1\n",
    "3. Fit the $W_{xo}$ and $W{ho}$ using the closed-form formula for least squares\n",
    "4. Since steps 1.-3. are super-fast, repeat them multiple times with different scaling of $W_{hh}$ and $W_{xh}$.\n",
    "\n",
    "Because $W_{hh}$ has the largest eigenvalue sligtly less than 1, the hidden states oscillate and decay slowly. This creates \"echoes\" of previous inputs that reverberate in the network.\n",
    "\n",
    "#### Gradient clipping\n",
    "\n",
    "Gradient explosion can be prevented by rescaling gradients that are larger than a specified threshold. You can either clip individual components of the gradient, or rescale the whole gradient.\n",
    "\n",
    "#### LSTM cells\n",
    "\n",
    "LSTM's are important! They contributed a lot to recent sucesses of recurrent neural networks!\n",
    "\n",
    "For introduction and intuitions please see http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "For more in-depth analysis please look at: http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf and http://arxiv.org/pdf/1503.04069v1.pdf\n",
    "\n",
    "The core idea of LSTM is to introduce multiplicative *gates* that enable long pathways of nearly constant values/gradients of memory cells that we will denote $c$. The core equations of LSTM are:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    i_t &= \\sigma(W_{xi}X_t + W_{hi}H_{t-1} + W_{ci}c_{t-1} + b_i) \\\\\n",
    "    f_t &= \\sigma(W_{xf}X_t + W_{hf}H_{t-1} + W_{cf}c_{t-1} + b_f) \\\\\n",
    "    c_t &= f_t c_{t-1} + i_t\\tanh(W_{xc}X_t + W_{hc}H_{t-1} + b_i) \\\\\n",
    "    o_t &= \\sigma(W_{xo}X_t + W_{ho}H_{t-1} + W_{co}c_{t} + b_o) \\\\\n",
    "    H_t &= o_t \\tanh(c_t)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### Multilayer and bi-directional LSTM networks\n",
    "\n",
    "It is possible to stack several LSTM layers (see the code below). Likewise, it is common to invert the input sequence to run the LSTM backward in time. Then the forward and backward hidden states give a summary of the sequence around a certain element.\n",
    "\n",
    "### Tricks of the trade\n",
    "\n",
    "1. Use train rules that allow per-parameter learnign rates (e.g. RMSProp)\n",
    "2. Monitor gradient magnitude!\n",
    "3. Initialization is important:\n",
    "    - it often helps to orthogonalize recurent weights and rescale to have the largest eigenvalue close to 1. This is similar to hidden-to-hidden weights in echo state networks\n",
    "    - forget-gate biases in LSTMs are ofetn initialized to 1 instead of 0. This enhances information retention at the beginning of training\n",
    "4. Learning interdependencies across many time steps is difficult. If possible train on short (or othervise simple) sequences first (this is often called \"curriculum learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "debug = True # global var to control debugging\n",
    "CUDA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.tensor as T\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "from torch.nn import functional as F\n",
    "\n",
    "def V(np_var, **kwargs):\n",
    "    t = torch.from_numpy(np_var)\n",
    "    if CUDA:\n",
    "        t = t.cuda()\n",
    "    return Variable(t, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN implementation in PyTorch\n",
    "\n",
    "Please note that PyTorch has RNN implementations (http://pytorch.org/docs/master/nn.html#recurrent-layers). The goal of this notebook is to demonstrate the basics of RNNs. For future projects please use the built-in mudules. They have more features, and are faster (they use the CuDNN library supplied by Nvidia).\n",
    "\n",
    "**Attention**: through the code we will assume that the 0-th axis refers to time and that the 1-st axis refers to individual examples inside a minibatch. (This way in a C-major memory layout individual time steps occupy contiguous regions in memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNNCell(torch.nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, activation=torch.nn.Tanh(), **kwargs):\n",
    "        super(SimpleRNNCell, self).__init__(**kwargs)\n",
    "        \n",
    "        self.activation = activation\n",
    "        # Input to hidden\n",
    "        self.Wxh = Parameter(torch.Tensor(in_dim, hidden_dim))\n",
    "        \n",
    "        # Hidden to hidden\n",
    "        self.Whh = Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "        \n",
    "        # Hidden bias\n",
    "        self.bh = Parameter(torch.Tensor(hidden_dim))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.Whh.size(1))\n",
    "        self.Wxh.data.uniform_(-stdv, stdv)\n",
    "        self.Whh.data.uniform_(-stdv, stdv)\n",
    "        self.bh.data.zero_()\n",
    "    \n",
    "    def forward(self, input, h):\n",
    "        pre_act = (\n",
    "            self.bh +\n",
    "            torch.matmul(input, self.Wxh) + \n",
    "            torch.matmul(h, self.Whh)\n",
    "        )\n",
    "        return self.activation(pre_act)\n",
    "            \n",
    "\n",
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self, cell, **kwargs):\n",
    "        super(RNN, self).__init__(**kwargs)\n",
    "        self.cell = cell\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        cell = self.cell\n",
    "        output = []\n",
    "        for i in range(input.size(0)):\n",
    "            hidden = cell(input[i], hidden)\n",
    "            # hack to handle LSTM\n",
    "            output.append(hidden[0] if isinstance(hidden, tuple) else hidden)\n",
    "\n",
    "        output = torch.cat(output, 0).view(input.size(0), *output[0].size())\n",
    "\n",
    "        return hidden, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The parity task\n",
    "\n",
    "Here we solve the bit parity problem. Note that we will need at least two hidden neurons, because the network has to solve a XOR problem at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.T: [[ 1.  1.  0.  0.  1.  1.  1.  1.  1.  1.  0.  1.  1.  0.  1.  1.  1.  0.]] \n",
      "Y.T: [[ 1.  0.  0.  0.  1.  0.  1.  0.  1.  0.  0.  1.  0.  0.  1.  0.  1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "def gen_parity_examples(time_steps, batch_size):\n",
    "    X = (numpy.random.rand(time_steps, batch_size, 1)>0.5).astype('float32')\n",
    "    Y = X.cumsum(0) % 2\n",
    "    return X,Y\n",
    "\n",
    "\n",
    "Xp,Yp = gen_parity_examples(18, 1)\n",
    "print('X.T:', Xp.reshape(Xp.shape[:-1]).T, '\\nY.T:', Yp.reshape(Yp.shape[:-1]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 1, 1)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O Variable containing:\n",
      "-0.0942 -0.2125\n",
      " 0.0058 -0.3019\n",
      " 0.0601 -0.1213\n",
      "-0.0099 -0.0431\n",
      "-0.0794 -0.2301\n",
      " 0.0008 -0.3070\n",
      "-0.0303 -0.3277\n",
      "-0.0076 -0.3380\n",
      "-0.0188 -0.3397\n",
      "-0.0118 -0.3413\n",
      " 0.0787 -0.1388\n",
      "-0.1111 -0.2582\n",
      " 0.0254 -0.3202\n",
      " 0.0524 -0.1267\n",
      "-0.0983 -0.2561\n",
      " 0.0174 -0.3182\n",
      "-0.0377 -0.3303\n",
      " 0.0915 -0.1370\n",
      "[torch.FloatTensor of size 18x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The input variable - a 3D tensor with axes:\n",
    "# time x batch_size x num_features\n",
    "\n",
    "hidden_dim = 2\n",
    "test_net = RNN(SimpleRNNCell(in_dim=1, hidden_dim=hidden_dim))\n",
    "\n",
    "h0 = np.zeros((1, hidden_dim), dtype='float32')  # 1 sequence in batch, 1 hidden dim\n",
    "hn, O = test_net(V(Xp), V(h0))\n",
    "\n",
    "print('O', O.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ParityNet(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim, **kwargs):\n",
    "        super(ParityNet, self).__init__(**kwargs)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.rnn = RNN(\n",
    "            cell=SimpleRNNCell(in_dim=1, hidden_dim=hidden_dim,\n",
    "                               activation=torch.nn.Sigmoid()))\n",
    "        self.linear = torch.nn.Linear(in_features=1 + hidden_dim,\n",
    "                                      out_features=1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        h0 = np.zeros((inputs.size(1), self.hidden_dim),\n",
    "                      dtype='float32')\n",
    "        _, H = self.rnn(inputs, V(h0))\n",
    "        # concatenate inputs and hidden states\n",
    "        inputs_and_H = torch.cat((inputs, H), dim=2)\n",
    "        return F.sigmoid(self.linear(inputs_and_H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        X:  [ 1.  1.  0.  0.  1.  1.  1.  1.  1.  1.  0.  1.  1.  0.  1.  1.  1.  0.]\n",
      "predicted:  [ 1.  0.  0.  0.  1.  0.  1.  0.  1.  0.  0.  1.  0.  0.  1.  0.  1.  1.]\n",
      "        Y:  [ 1.  0.  0.  0.  1.  0.  1.  0.  1.  0.  0.  1.  0.  0.  1.  0.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "parity_net = ParityNet(hidden_dim=2)\n",
    "\n",
    "#\n",
    "# Design the net to solve parity\n",
    "#\n",
    "parity_net.rnn.cell.Wxh.data[...\n",
    "    ] = torch.FloatTensor([[2000, 1000]])\n",
    "parity_net.rnn.cell.Whh.data[...\n",
    "    ] = torch.FloatTensor([[ 1000,  1000],\n",
    "                           [-1000, -1000]])\n",
    "parity_net.rnn.cell.bh.data[...\n",
    "    ] = torch.FloatTensor([-500, -1500])\n",
    "\n",
    "parity_net.linear.weight.data[...\n",
    "    ] = torch.FloatTensor([[ 0],\n",
    "                           [ 1000],\n",
    "                           [-1000]])\n",
    "parity_net.linear.bias.data[...\n",
    "    ] = torch.FloatTensor([-500])\n",
    "    \n",
    "predictions = parity_net(V(Xp)).data.numpy().squeeze()\n",
    "print('        X: ', Xp.squeeze())\n",
    "print('predicted: ', predictions)\n",
    "print('        Y: ', Yp.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, network):\n",
    "        # If full_supervision == True, supervice net outputs at all steps\n",
    "        # if full_supervision == False, supervise net outputs at the last\n",
    "        # step only\n",
    "        self.full_supervision = True\n",
    "        \n",
    "        # When the total gradient is above this value, scale it down\n",
    "        self.max_grad_norm = 1.0\n",
    "        \n",
    "        self.network = network\n",
    "        \n",
    "        # RMSprop stuff\n",
    "        self.lrate = 1e-2\n",
    "        self.RMSProp_dec_rate = 0.9\n",
    "        self.RMSProp_epsilon = 1e-5\n",
    "        self.gnorms = [torch.zeros(*p.size())\n",
    "                       for p in self.network.parameters()]\n",
    "    \n",
    "    def train_step(self, X, Y):\n",
    "        X = V(X)\n",
    "        Y = V(Y)\n",
    "        net = self.network\n",
    "        net.zero_grad()\n",
    "        # predictions\n",
    "        P = net(X)\n",
    "        if self.full_supervision:\n",
    "            loss = torch.mean((P - Y)**2)\n",
    "        else:\n",
    "            loss = torch.mean((P[-1] - Y[-1])**2)\n",
    "        loss.backward()\n",
    "        \n",
    "        tot_gnorm = torch.FloatTensor((0.0,))\n",
    "        # compute the norm of the gradient\n",
    "        for p in net.parameters():\n",
    "            tot_gnorm += torch.sum(p.grad.data**2)\n",
    "        tot_gnorm = torch.sqrt(tot_gnorm)\n",
    "        \n",
    "        g_div = torch.max(torch.FloatTensor((self.max_grad_norm,)), tot_gnorm)\n",
    "        \n",
    "        for i, p in enumerate(net.parameters()):\n",
    "            g = p.grad.data / g_div\n",
    "            #TODO: weight decay\n",
    "            self.gnorms[i] *= self.RMSProp_dec_rate\n",
    "            self.gnorms[i] += (1.0 - self.RMSProp_dec_rate) * g**2\n",
    "            p.data -= self.lrate * g / torch.sqrt(self.gnorms[i] + self.RMSProp_epsilon)\n",
    "        return loss.data[0], tot_gnorm[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration of various problems with RNN training\n",
    "\n",
    "In the following cell you can implement with various ways of providing supervision to the network (which one is easier to train - when the net recieves feedback after each step or whn it recieves feedback only at the end of training?), changing the gradient clipping and playing with a curriculum.\n",
    "\n",
    "Notice the dynamics of training - at the beginning the network does very little. The, suddenly it notices the input-output relationship from which point training starts to progress very quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (0.25732627511024475, 0.08379088342189789)\n",
      "500 (0.25693824887275696, 0.04508886858820915)\n",
      "1000 (0.24951350688934326, 0.006582588888704777)\n",
      "1500 (0.24370935559272766, 0.12872189283370972)\n",
      "2000 (0.24369142949581146, 0.008430727757513523)\n",
      "2500 (0.22357340157032013, 0.03397641330957413)\n",
      "3000 (0.20257297158241272, 0.02979537472128868)\n",
      "3500 (0.19381365180015564, 0.02311432920396328)\n",
      "4000 (0.17163842916488647, 0.03057691641151905)\n",
      "4500 (0.0020587092731148005, 0.001996203325688839)\n",
      "5000 (0.0006739101954735816, 0.0007265237509272993)\n",
      "5500 (0.00034291495103389025, 0.0004969764850102365)\n",
      "6000 (0.00029793783323839307, 0.0002611169184092432)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-43278ef13ad6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mXp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_parity_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparity_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-0c02d5fd5a95>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mg_div\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtot_gnorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mg_div\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;31m#TODO: weight decay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jch/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mparameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0;34m<\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0;34m'torch.FloatTensor'\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \"\"\"\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jch/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mnamed_parameters\u001b[0;34m(self, memo, prefix)\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_children\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m             \u001b[0msubmodule_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubmodule_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jch/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mnamed_parameters\u001b[0;34m(self, memo, prefix)\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_children\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m             \u001b[0msubmodule_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubmodule_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jch/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mnamed_parameters\u001b[0;34m(self, memo, prefix)\u001b[0m\n\u001b[1;32m    398\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_children\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m             \u001b[0msubmodule_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubmodule_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parity_net = ParityNet(hidden_dim=10)\n",
    "parity_trainer = Trainer(parity_net)\n",
    "\n",
    "# you can tweek the learning rate. 1e-2 worked best for me\n",
    "parity_trainer.lrate = 1e-2\n",
    "\n",
    "losses = []\n",
    "\n",
    "# when set to 1 the net recieves an error signal after each step\n",
    "# when set to 0 the net recieves an error signal only once at the end\n",
    "parity_trainer.full_sup = False\n",
    "\n",
    "# without full_supervision it doesn't train for sequences longer than 3\n",
    "seq_len = 10\n",
    "\n",
    "# this enables \"curriculum learning\" - we gradually train on \n",
    "# longer and longer sequences\n",
    "#\n",
    "max_seq_len = 1\n",
    "\n",
    "for i in range(100000):\n",
    "    Xp, Yp = gen_parity_examples(seq_len, 10)\n",
    "    ret = parity_trainer.train_step(Xp, Yp)\n",
    "    losses.append((i,) + tuple(ret))\n",
    "    if ret[0]<1e-4:\n",
    "        seq_len += 1\n",
    "        if seq_len>max_seq_len:\n",
    "            break\n",
    "        print(i, \"Increasing seq length to: \", seq_len)\n",
    "    if i%500 == 0:\n",
    "        print(i, ret)\n",
    "    \n",
    "losses_a = np.array(losses)\n",
    "\n",
    "semilogy(losses_a[:,0], losses_a[:,1], label='loss')\n",
    "semilogy(losses_a[:,0], losses_a[:,2], label='grad norm', alpha=0.5)\n",
    "\n",
    "legend(loc='lower left')\n",
    "title('Training loss')\n",
    "xlabel('iteration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        X:  [ 1.  1.  1.  0.  1.  0.  0.  0.  0.  1.  1.  0.  0.  0.  1.  0.  0.  0.]\n",
      "predicted:  [ 1.  0.  0.  1.  1.  1.  0.  0.  0.  1.  0.  1.  0.  0.  1.  1.  0.  0.]\n",
      "        Y:  [ 1.  0.  1.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "Xp,Yp = gen_parity_examples(18, 1)\n",
    "predictions = parity_net(V(Xp)).data.numpy().squeeze()\n",
    "print('        X: ', Xp.squeeze())\n",
    "print('predicted: ', np.round(predictions))\n",
    "print('        Y: ', Yp.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 22996.8\n",
      "err_rate 0.49676\n"
     ]
    }
   ],
   "source": [
    "Xp, Yp = gen_parity_examples(500, 100)\n",
    "P = parity_net(V(Xp))\n",
    "print('loss', np.sum((Yp.squeeze() - P.data.numpy().squeeze())**2))\n",
    "print('err_rate', (Yp.squeeze() != np.round(P.data.numpy().squeeze())).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addition task\n",
    "\n",
    "Here we explore the task of adding two inputs marked by a binary inicators. This is a difficult task, because the net must learn to ignore the spurious inputs and to discover the relationship between the desired output and two distant time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [[ 1.          0.47704914]\n",
      " [ 0.          0.44842863]\n",
      " [ 1.          0.23004869]\n",
      " [ 0.          0.62760085]\n",
      " [ 0.          0.19760039]\n",
      " [ 0.          0.36385655]\n",
      " [ 0.          0.63792521]\n",
      " [ 0.          0.46334672]\n",
      " [ 0.          0.56075335]\n",
      " [ 0.          0.22399838]] \n",
      "Y: [ 0.35354891]\n"
     ]
    }
   ],
   "source": [
    "# adapted from \n",
    "# https://github.com/pascanur/trainingRNNs/blob/master/addition.py\n",
    "def gen_addition_example(T, batchsize):\n",
    "    rng = numpy.random\n",
    "    \n",
    "    l = rng.randint(T, int(T * 1.1+0.9))\n",
    "    p0 = rng.randint(0, int(l*.1), size=(batchsize,))\n",
    "    p1 = rng.randint(0, int(l*.4), size=(batchsize,)) + int(l*.1)\n",
    "    \n",
    "    X = rng.uniform(size=(l, batchsize, 2)).astype('float32')\n",
    "    X[:,:,0] = 0.\n",
    "    X[p0, numpy.arange(batchsize), numpy.zeros((batchsize,),\n",
    "                                                dtype='int32')] = 1.\n",
    "    X[p1, numpy.arange(batchsize), numpy.zeros((batchsize,),\n",
    "                                                dtype='int32')] = 1.\n",
    "\n",
    "    Y = (X[p0, numpy.arange(batchsize),\n",
    "           numpy.ones((batchsize,), dtype='int32')] + \\\n",
    "         X[p1, numpy.arange(batchsize),\n",
    "           numpy.ones((batchsize,), dtype='int32')])/2.\n",
    "    return X, Y.reshape((1, -1, 1)).astype('float32')\n",
    "\n",
    "Xa,Ya = gen_addition_example(10, 3)\n",
    "print('X:', Xa[:,0,:], '\\nY:', Ya[0, 0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTMCell(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation follows Alex Graves, Abdel-rahman Mohamed and Geoffrey Hinton\n",
    "    \"SPEECH RECOGNITION WITH DEEP RECURRENT NEURAL NETWORKS\"\n",
    "    http://www.cs.toronto.edu/~fritz/absps/RNN13.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, hidden_dim, **kwargs):\n",
    "        super(LSTMCell, self).__init__(**kwargs)\n",
    "        self.in_to_gates = torch.nn.Linear(in_dim, hidden_dim * 4, bias=True)\n",
    "        self.hidden_to_gates = torch.nn.Linear(hidden_dim, hidden_dim * 4, bias=False)\n",
    "    \n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        # TODO: add connections from the cell values\n",
    "        hx, cx = hidden\n",
    "        gates = self.in_to_gates(input) + self.hidden_to_gates(hx)\n",
    "        \n",
    "        ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n",
    "        ingate = F.sigmoid(ingate)\n",
    "        forgetgate = F.sigmoid(forgetgate + 1.0)\n",
    "        cellgate = F.tanh(cellgate)\n",
    "        outgate = F.sigmoid(outgate)\n",
    "\n",
    "        cy = (forgetgate * cx) + (ingate * cellgate)\n",
    "        hy = outgate * F.tanh(cy)\n",
    "        return hy, cy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AdditionNet(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim, use_lstm=True,\n",
    "                 **kwargs):\n",
    "        super(AdditionNet, self).__init__(**kwargs)\n",
    "        self.use_lstm = use_lstm\n",
    "        self.recs = []\n",
    "        \n",
    "        self.use_lstm = use_lstm\n",
    "        self.hidden_dim = hidden_dim\n",
    "        in_dim = 2\n",
    "        if use_lstm:\n",
    "            cell = LSTMCell(in_dim=in_dim, hidden_dim=hidden_dim)\n",
    "        else:\n",
    "            cell = SimpleRNNCell(in_dim=in_dim, hidden_dim=hidden_dim)\n",
    "        self.rnn = RNN(cell)\n",
    "        \n",
    "        self.linear = torch.nn.Linear(in_dim + hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        if self.use_lstm:\n",
    "            h0 = [V(np.zeros((inputs.size(1), self.hidden_dim),\n",
    "                              dtype='float32'))\n",
    "                 for _ in range(2)]\n",
    "        else:\n",
    "            h0 = V(np.zeros((inputs.size(1), self.hidden_dim),\n",
    "                             dtype='float32'))\n",
    "        _, H = self.rnn(inputs, h0)\n",
    "        # concatenate inputs and hidden states\n",
    "        inputs_and_H = torch.cat((inputs, H), dim=2)\n",
    "        return self.linear(inputs_and_H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.042333862524130746"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How large an error do we expect?\n",
    "\n",
    "((np.random.rand(2, 100).sum(0) * 0.5 - 0.5)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (0.29333147406578064, 1.788282871246338)\n",
      "200 (0.02824277989566326, 0.09064994007349014)\n",
      "400 (0.022798040881752968, 0.44161397218704224)\n",
      "600 (0.008114969357848167, 0.19528204202651978)\n",
      "800 (0.01131235808134079, 0.43403348326683044)\n",
      "1000 (0.0038368389941751957, 0.22313722968101501)\n",
      "1200 (0.0011022104881703854, 0.1289232075214386)\n",
      "1400 (0.0010852600680664182, 0.07656286656856537)\n",
      "1600 (0.0024847988970577717, 0.2423761636018753)\n",
      "1800 (0.00037326160236261785, 0.007126002572476864)\n",
      "2000 (0.004979523830115795, 0.3844037353992462)\n",
      "2200 (0.0005503584397956729, 0.03304217383265495)\n",
      "2400 (0.0007863313076086342, 0.06580955535173416)\n",
      "2600 (0.0015656036557629704, 0.16541653871536255)\n",
      "2800 (0.0015155250439420342, 0.1558847427368164)\n",
      "3000 (0.003407458309084177, 0.212800994515419)\n",
      "3200 (0.0034086687956005335, 0.2860262095928192)\n",
      "3400 (0.001506257918663323, 0.13912461698055267)\n",
      "3600 (0.0008487891755066812, 0.11162377893924713)\n",
      "3800 (0.001071209437213838, 0.13396330177783966)\n",
      "4000 (0.0001568526349728927, 0.01530463621020317)\n",
      "4200 (0.00022644932323601097, 0.0385984368622303)\n",
      "4400 (0.0006098942249082029, 0.08688578009605408)\n",
      "4552 Increasing seq length to:  25\n",
      "4600 (0.0012352456105872989, 0.1454736888408661)\n",
      "4800 (0.00020324763318058103, 0.03627166152000427)\n",
      "5000 (0.0013352022506296635, 0.16404473781585693)\n",
      "5200 (0.004868864547461271, 0.29549694061279297)\n",
      "5400 (0.0013690005289390683, 0.1333848387002945)\n",
      "5600 (0.0003806452441494912, 0.0534818209707737)\n",
      "5715 Increasing seq length to:  30\n",
      "5800 (0.00042644410859793425, 0.06962330639362335)\n",
      "6000 (0.00017695101269055158, 0.01228462252765894)\n",
      "6200 (0.001134903752245009, 0.12148590385913849)\n",
      "6400 (0.003219770500436425, 0.2369624227285385)\n",
      "6413 Increasing seq length to:  35\n",
      "6600 (0.0004538220528047532, 0.054878540337085724)\n",
      "6690 Increasing seq length to:  40\n",
      "6800 (0.00034078824683092535, 0.05169210955500603)\n",
      "7000 (0.0013634150382131338, 0.05434112623333931)\n",
      "7200 (0.0002833690959960222, 0.012322762981057167)\n",
      "7400 (0.00042968703201040626, 0.05154545232653618)\n",
      "7600 (0.002058387268334627, 0.15569937229156494)\n",
      "7800 (0.000192257619346492, 0.024197425693273544)\n",
      "8000 (0.00022682060080114752, 0.02528333105146885)\n",
      "8200 (0.0006620926433242857, 0.09386477619409561)\n",
      "8400 (0.0008387150592170656, 0.0883481502532959)\n",
      "8596 Increasing seq length to:  45\n",
      "8600 (0.0006431101937778294, 0.11165616661310196)\n",
      "8800 (0.0004399361787363887, 0.013869119808077812)\n",
      "9000 (0.00018597875896375626, 0.02063874527812004)\n",
      "9150 Increasing seq length to:  50\n",
      "9200 (0.00020388343546073884, 0.0036448403261601925)\n",
      "9400 (0.0003157473402097821, 0.04810507595539093)\n",
      "9600 (0.002094917930662632, 0.1933705359697342)\n",
      "9800 (0.0006664256216026843, 0.08447370678186417)\n",
      "10000 (0.0010346476919949055, 0.11067233979701996)\n",
      "10200 (0.00023087093723006546, 0.023546818643808365)\n",
      "10400 (0.001634139334782958, 0.03668975830078125)\n",
      "10600 (0.0002823065151460469, 0.061867110431194305)\n",
      "10800 (0.00013120438961777836, 0.012295546010136604)\n",
      "11000 (0.00012035672989441082, 0.012758427299559116)\n",
      "11039 Increasing seq length to:  55\n",
      "11200 (0.0005988843040540814, 0.08109930902719498)\n",
      "11362 Increasing seq length to:  60\n",
      "11400 (0.0002428664593026042, 0.0321333147585392)\n",
      "11600 (0.00013286594185046852, 0.012760858982801437)\n",
      "11765 Increasing seq length to:  65\n",
      "11800 (0.0005284918588586152, 0.06737330555915833)\n",
      "12000 (0.0022587229032069445, 0.18199893832206726)\n",
      "12028 Increasing seq length to:  70\n",
      "12200 (0.00046524277422577143, 0.07215200364589691)\n",
      "12247 Increasing seq length to:  75\n",
      "12400 (0.005130684934556484, 0.21884015202522278)\n",
      "12600 (0.0009214745368808508, 0.11677952855825424)\n",
      "12800 (0.00013101364311296493, 0.005422488320618868)\n",
      "12990 Increasing seq length to:  80\n",
      "13000 (0.001965161180123687, 0.15497355163097382)\n",
      "13200 (0.00043371893116272986, 0.048139218240976334)\n",
      "13400 (0.0007119388901628554, 0.08254066854715347)\n",
      "13592 Increasing seq length to:  85\n",
      "13600 (0.0004079030768480152, 0.0743820071220398)\n",
      "13800 (0.00033060775604099035, 0.0225111972540617)\n",
      "13948 Increasing seq length to:  90\n",
      "14000 (0.0018567306688055396, 0.18687990307807922)\n",
      "14106 Increasing seq length to:  95\n",
      "14200 (0.00016366792260669172, 0.016180867329239845)\n",
      "14400 (0.000404689519200474, 0.049299053847789764)\n",
      "14471 Increasing seq length to:  100\n",
      "14600 (0.00019803718896582723, 0.02091192826628685)\n",
      "14800 (0.0005568555206991732, 0.08561602234840393)\n",
      "14915 Increasing seq length to:  105\n",
      "14973 Increasing seq length to:  110\n",
      "15000 (0.0018255810718983412, 0.1738843470811844)\n",
      "15049 Increasing seq length to:  115\n",
      "15200 (0.00042922273860313, 0.04830048233270645)\n",
      "15327 Increasing seq length to:  120\n",
      "15400 (0.00027800985844805837, 0.03698224201798439)\n",
      "15600 (0.006621663924306631, 0.41287049651145935)\n",
      "15800 (0.0009255870245397091, 0.1272912323474884)\n",
      "16000 (0.0014363487716764212, 0.1429402381181717)\n",
      "16200 (0.0003270000161137432, 0.0431467741727829)\n",
      "16366 Increasing seq length to:  125\n",
      "16400 (0.00038420723285526037, 0.04861696809530258)\n",
      "16600 (0.00022306691971607506, 0.0066935778595507145)\n",
      "16613 Increasing seq length to:  130\n",
      "16637 Increasing seq length to:  135\n",
      "16800 (0.00013625447172671556, 0.012882145121693611)\n",
      "16900 Increasing seq length to:  140\n",
      "17000 (0.000783295719884336, 0.12659244239330292)\n",
      "17200 (0.000474836298963055, 0.06229844689369202)\n",
      "17285 Increasing seq length to:  145\n",
      "17400 (0.0006626973627135158, 0.07962077111005783)\n",
      "17600 (0.0016699411207810044, 0.16675274074077606)\n",
      "17666 Increasing seq length to:  150\n",
      "17790 Increasing seq length to:  155\n",
      "17800 (0.005106846336275339, 0.257977694272995)\n",
      "17846 Increasing seq length to:  160\n",
      "18000 (0.00037193280877545476, 0.06440674513578415)\n",
      "18042 Increasing seq length to:  165\n",
      "18200 (0.00035470243892632425, 0.04381584748625755)\n",
      "18398 Increasing seq length to:  170\n",
      "18400 (0.00021379909594543278, 0.027914274483919144)\n",
      "18430 Increasing seq length to:  175\n",
      "18600 (0.0004458767652977258, 0.08250121772289276)\n",
      "18800 (0.001936924527399242, 0.1441662609577179)\n",
      "18842 Increasing seq length to:  180\n",
      "19000 (0.004193097818642855, 0.3100302517414093)\n",
      "19050 Increasing seq length to:  185\n",
      "19098 Increasing seq length to:  190\n",
      "19200 (0.0008806138066574931, 0.07363487780094147)\n",
      "19400 (0.0013086766703054309, 0.10827350616455078)\n",
      "19600 (0.0010992176830768585, 0.07959914207458496)\n",
      "19762 Increasing seq length to:  195\n",
      "19776 Increasing seq length to:  200\n",
      "19800 (0.0026303951162844896, 0.21561947464942932)\n",
      "20000 (0.0008807100239209831, 0.09433677792549133)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7fe9504742b0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEWCAYAAABv+EDhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWAEwDMNEKf8PqjxQZez5Y+sAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe950337cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "addition_net = AdditionNet(hidden_dim=20, use_lstm=True)\n",
    "addition_trainer = Trainer(addition_net)\n",
    "addition_trainer.full_supervision = False\n",
    "\n",
    "# you can tweek the learning rate.\n",
    "if addition_net.use_lstm:\n",
    "    addition_trainer.lrate = 5e-3\n",
    "else:\n",
    "    addition_trainer.lrate= 1e-3\n",
    "addition_trainer.max_grad_norm = 3.\n",
    "# weight decay seems to be important for generalization\n",
    "# addition_trainer.wdec.set_value(1e-4)\n",
    "\n",
    "losses = []\n",
    "\n",
    "# without full_supervision it doesn't train for sequences longer than 3\n",
    "seq_len = 20\n",
    "\n",
    "# this enables \"curriculum learning\" - we gradually train on \n",
    "# longer and longer sequences\n",
    "#\n",
    "max_seq_len = 200\n",
    "\n",
    "for i in range(100000):\n",
    "    # note: we need to train on sequences of all lengths in order to\n",
    "    # prevent forgetting the solution on short sequences\n",
    "    this_len = np.random.randint(10, seq_len+1)\n",
    "    Xa, Ya = gen_addition_example(this_len, 100)\n",
    "    ret = addition_trainer.train_step(Xa, Ya)\n",
    "    losses.append((i,) + tuple(ret))\n",
    "    if this_len>seq_len*0.9 and ret[0] < 0.0002:\n",
    "        seq_len += 5\n",
    "        if seq_len>max_seq_len:\n",
    "            break\n",
    "        print(i, \"Increasing seq length to: \", seq_len)\n",
    "    if i%200 == 0:\n",
    "        print(i, ret)\n",
    "    \n",
    "losses_a = np.array(losses)\n",
    "\n",
    "semilogy(losses_a[:,0], losses_a[:,1], label='rms')\n",
    "semilogy(losses_a[:,0], losses_a[:,2], alpha=0.5, label='grad norm')\n",
    "\n",
    "legend(loc='lower left')\n",
    "title('Training loss')\n",
    "xlabel('iteration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0010822577"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xa, Ya = gen_addition_example(1000,2000)\n",
    "P = addition_net(V(Xa))\n",
    "((P.data.numpy()[-1] - Ya[-1])**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
